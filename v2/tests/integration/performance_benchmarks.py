"""
æ€§èƒ½åŸºå‡†æµ‹è¯•æ¨¡å—

æä¾›å…¨é¢çš„æ€§èƒ½ç›‘æ§å’ŒåŸºå‡†æµ‹è¯•åŠŸèƒ½ï¼ŒåŒ…æ‹¬å»¶è¿Ÿã€ååé‡ã€å†…å­˜ä½¿ç”¨ç­‰æŒ‡æ ‡ã€‚
"""

import time
import psutil
import threading
import statistics
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, field
from enum import Enum
import json
import logging
from pathlib import Path
import gc
import sys

# æ·»åŠ v2ç›®å½•åˆ°Pythonè·¯å¾„
v2_path = Path(__file__).parent.parent.parent
sys.path.insert(0, str(v2_path))

from v2.controller.poker_controller import PokerController
from v2.core.state import GameState
from v2.core.enums import ActionType, Action, SeatStatus
from v2.core.player import Player


class BenchmarkType(Enum):
    """åŸºå‡†æµ‹è¯•ç±»å‹"""
    LATENCY = "latency"
    THROUGHPUT = "throughput"
    MEMORY = "memory"
    STRESS = "stress"
    ENDURANCE = "endurance"


@dataclass
class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†"""
    name: str
    benchmark_type: BenchmarkType
    target_value: float
    tolerance: float = 0.1  # 10% tolerance
    unit: str = "ms"
    description: str = ""


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    benchmark_type: BenchmarkType
    measured_value: float
    passed: bool
    threshold: float
    duration: float
    timestamp: float = field(default_factory=time.time)
    metadata: Dict[str, Any] = field(default_factory=dict)


class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.process = psutil.Process()
        self.monitoring = False
        self.metrics_history: List[Dict[str, Any]] = []
        self._monitor_thread: Optional[threading.Thread] = None
        self._lock = threading.Lock()
    
    def start_monitoring(self, interval: float = 0.1):
        """å¼€å§‹æ€§èƒ½ç›‘æ§"""
        if self.monitoring:
            return
        
        self.monitoring = True
        self._monitor_thread = threading.Thread(target=self._monitor_loop, args=(interval,))
        self._monitor_thread.daemon = True
        self._monitor_thread.start()
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=1.0)
    
    def _monitor_loop(self, interval: float):
        """ç›‘æ§å¾ªç¯"""
        while self.monitoring:
            try:
                metrics = self._collect_metrics()
                with self._lock:
                    self.metrics_history.append(metrics)
                time.sleep(interval)
            except Exception as e:
                logging.error(f"Error in performance monitoring: {e}")
    
    def _collect_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        try:
            memory_info = self.process.memory_info()
            cpu_percent = self.process.cpu_percent()
            
            return {
                "timestamp": time.time(),
                "memory_rss": memory_info.rss,
                "memory_vms": memory_info.vms,
                "cpu_percent": cpu_percent,
                "num_threads": self.process.num_threads(),
            }
        except Exception as e:
            logging.error(f"Error collecting metrics: {e}")
            return {"timestamp": time.time(), "error": str(e)}
    
    def get_summary(self) -> Dict[str, Any]:
        """è·å–ç›‘æ§æ‘˜è¦"""
        with self._lock:
            if not self.metrics_history:
                return {}
            
            # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
            valid_metrics = [m for m in self.metrics_history if "error" not in m]
            if not valid_metrics:
                return {"error": "No valid metrics collected"}
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            memory_rss_values = [m["memory_rss"] for m in valid_metrics]
            cpu_values = [m["cpu_percent"] for m in valid_metrics]
            
            return {
                "duration": valid_metrics[-1]["timestamp"] - valid_metrics[0]["timestamp"],
                "samples": len(valid_metrics),
                "memory": {
                    "avg_rss_mb": statistics.mean(memory_rss_values) / 1024 / 1024,
                    "max_rss_mb": max(memory_rss_values) / 1024 / 1024,
                    "min_rss_mb": min(memory_rss_values) / 1024 / 1024
                },
                "cpu": {
                    "avg_percent": statistics.mean(cpu_values),
                    "max_percent": max(cpu_values),
                    "min_percent": min(cpu_values)
                }
            }


class PerformanceBenchmarkSuite:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•å¥—ä»¶"""
    
    def __init__(self):
        self.benchmarks: List[PerformanceBenchmark] = []
        self.results: List[BenchmarkResult] = []
        self.monitor = PerformanceMonitor()
        self.logger = logging.getLogger(__name__)
        
        # è®¾ç½®é»˜è®¤åŸºå‡†
        self._setup_default_benchmarks()
    
    def _setup_default_benchmarks(self):
        """è®¾ç½®é»˜è®¤æ€§èƒ½åŸºå‡†"""
        self.benchmarks = [
            PerformanceBenchmark(
                "operation_latency",
                BenchmarkType.LATENCY,
                100.0,  # 100ms
                0.5,    # 50% tolerance
                "ms",
                "å•ä¸ªæ“ä½œçš„å¹³å‡å»¶è¿Ÿ"
            ),
            PerformanceBenchmark(
                "game_setup_time",
                BenchmarkType.LATENCY,
                50.0,   # 50ms
                0.5,    # 50% tolerance
                "ms",
                "æ¸¸æˆè®¾ç½®æ—¶é—´"
            ),
            PerformanceBenchmark(
                "action_execution_time",
                BenchmarkType.LATENCY,
                20.0,   # 20ms
                0.5,    # 50% tolerance
                "ms",
                "è¡ŒåŠ¨æ‰§è¡Œæ—¶é—´"
            ),
        ]
    
    def _create_test_controller(self) -> PokerController:
        """åˆ›å»ºæµ‹è¯•æ§åˆ¶å™¨"""
        game_state = GameState()
        
        # æ·»åŠ æµ‹è¯•ç©å®¶
        players = [
            Player(seat_id=0, name="Alice", chips=1000, status=SeatStatus.ACTIVE),
            Player(seat_id=1, name="Bob", chips=1000, status=SeatStatus.ACTIVE),
            Player(seat_id=2, name="Charlie", chips=1000, status=SeatStatus.ACTIVE),
            Player(seat_id=3, name="Diana", chips=1000, status=SeatStatus.ACTIVE)
        ]
        
        for player in players:
            game_state.add_player(player)
        
        # åˆå§‹åŒ–ç‰Œç»„
        game_state.initialize_deck()
        
        return PokerController(game_state=game_state)
    
    def run_benchmarks(self, controller: PokerController = None) -> List[BenchmarkResult]:
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        # æ€»æ˜¯åˆ›å»ºæ–°çš„æ§åˆ¶å™¨æ¥é¿å…çŠ¶æ€å†²çª
        # ä¼ å…¥çš„æ§åˆ¶å™¨å‚æ•°ä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼Œä½†ä¸ä½¿ç”¨
        
        results = []
        
        # è¿è¡Œå»¶è¿ŸåŸºå‡†æµ‹è¯•
        results.extend(self._run_latency_benchmarks(None))
        
        # è¿è¡Œå†…å­˜åŸºå‡†æµ‹è¯•
        results.extend(self._run_memory_benchmarks(None))
        
        self.results = results
        return results
    
    def _run_latency_benchmarks(self, controller: PokerController) -> List[BenchmarkResult]:
        """è¿è¡Œå»¶è¿ŸåŸºå‡†æµ‹è¯•"""
        results = []
        
        # æµ‹è¯•æ¸¸æˆè®¾ç½®æ—¶é—´
        setup_times = []
        for _ in range(5):
            start_time = time.time()
            test_controller = self._create_test_controller()
            setup_time = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
            setup_times.append(setup_time)
        
        avg_setup_time = statistics.mean(setup_times)
        setup_benchmark = next((b for b in self.benchmarks if b.name == "game_setup_time"), None)
        if setup_benchmark:
            passed = avg_setup_time <= setup_benchmark.target_value * (1 + setup_benchmark.tolerance)
            results.append(BenchmarkResult(
                benchmark_type=setup_benchmark.benchmark_type,
                measured_value=avg_setup_time,
                passed=passed,
                threshold=setup_benchmark.target_value,
                duration=avg_setup_time
            ))
        
        # æµ‹è¯•è¡ŒåŠ¨æ‰§è¡Œæ—¶é—´ - æ€»æ˜¯ä½¿ç”¨æ–°çš„æ§åˆ¶å™¨
        action_test_controller = self._create_test_controller()
        # ç¡®ä¿æ§åˆ¶å™¨çŠ¶æ€è¢«é‡ç½®
        if hasattr(action_test_controller, '_hand_in_progress'):
            action_test_controller._hand_in_progress = False
        action_test_controller.start_new_hand()
        action_times = []
        
        for _ in range(10):
            current_player = action_test_controller.get_current_player_id()
            if current_player is not None:
                start_time = time.time()
                action = Action(ActionType.CALL, 0, current_player)
                action_test_controller.execute_action(action)
                action_time = (time.time() - start_time) * 1000
                action_times.append(action_time)
            else:
                break
        
        if action_times:
            avg_action_time = statistics.mean(action_times)
            action_benchmark = next((b for b in self.benchmarks if b.name == "action_execution_time"), None)
            if action_benchmark:
                passed = avg_action_time <= action_benchmark.target_value * (1 + action_benchmark.tolerance)
                results.append(BenchmarkResult(
                    benchmark_type=action_benchmark.benchmark_type,
                    measured_value=avg_action_time,
                    passed=passed,
                    threshold=action_benchmark.target_value,
                    duration=avg_action_time
                ))
        
        return results
    
    def _run_memory_benchmarks(self, controller: PokerController) -> List[BenchmarkResult]:
        """è¿è¡Œå†…å­˜åŸºå‡†æµ‹è¯•"""
        results = []
        
        # ä½¿ç”¨æ–°çš„æ§åˆ¶å™¨é¿å…çŠ¶æ€å†²çª
        test_controller = self._create_test_controller()
        
        # å¼€å§‹å†…å­˜ç›‘æ§
        self.monitor.start_monitoring(0.1)
        
        try:
            # æ‰§è¡Œä¸€ç³»åˆ—æ“ä½œ
            for i in range(10):
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å¼€å§‹æ–°æ‰‹ç‰Œ
                if test_controller.is_hand_over():
                    # ç¡®ä¿æ§åˆ¶å™¨çŠ¶æ€è¢«é‡ç½®
                    if hasattr(test_controller, '_hand_in_progress'):
                        test_controller._hand_in_progress = False
                    test_controller.start_new_hand()
                
                # æ‰§è¡Œä¸€äº›è¡ŒåŠ¨
                for _ in range(5):
                    current_player = test_controller.get_current_player_id()
                    if current_player is not None:
                        action = Action(ActionType.CALL, 0, current_player)
                        test_controller.execute_action(action)
                    else:
                        break
                
                # å¦‚æœæ‰‹ç‰Œç»“æŸï¼Œç»§ç»­ä¸‹ä¸€è½®
                if test_controller.is_hand_over():
                    continue
            
            time.sleep(1.0)  # ç­‰å¾…ç›‘æ§æ•°æ®
            
        finally:
            self.monitor.stop_monitoring()
        
        # åˆ†æå†…å­˜ä½¿ç”¨
        summary = self.monitor.get_summary()
        if summary and "memory" in summary:
            avg_memory_mb = summary["memory"]["avg_rss_mb"]
            
            # åˆ›å»ºå†…å­˜åŸºå‡†ç»“æœ
            memory_threshold = 200.0  # 200MBé˜ˆå€¼
            passed = avg_memory_mb <= memory_threshold
            results.append(BenchmarkResult(
                benchmark_type=BenchmarkType.MEMORY,
                measured_value=avg_memory_mb,
                passed=passed,
                threshold=memory_threshold,
                duration=summary["duration"] * 1000,  # è½¬æ¢ä¸ºæ¯«ç§’
                metadata={"max_memory_mb": summary["memory"]["max_rss_mb"]}
            ))
        
        return results
    
    def get_benchmark_summary(self) -> Dict[str, Any]:
        """è·å–åŸºå‡†æµ‹è¯•æ‘˜è¦"""
        if not self.results:
            return {"error": "No benchmark results available"}
        
        total_benchmarks = len(self.results)
        passed_benchmarks = sum(1 for r in self.results if r.passed)
        
        return {
            "total_benchmarks": total_benchmarks,
            "passed_benchmarks": passed_benchmarks,
            "failed_benchmarks": total_benchmarks - passed_benchmarks,
            "success_rate": passed_benchmarks / total_benchmarks if total_benchmarks > 0 else 0,
            "results": [
                {
                    "type": r.benchmark_type.value,
                    "measured_value": r.measured_value,
                    "threshold": r.threshold,
                    "passed": r.passed,
                    "duration": r.duration
                }
                for r in self.results
            ]
        }


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    suite = PerformanceBenchmarkSuite()
    results = suite.run_benchmarks()
    
    print("ğŸ¯ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ")
    print("=" * 50)
    
    for result in results:
        status = "âœ…" if result.passed else "âŒ"
        print(f"{status} {result.benchmark_type.value}: {result.measured_value:.2f}ms (é˜ˆå€¼: {result.threshold:.2f}ms)")
    
    summary = suite.get_benchmark_summary()
    print(f"\næ€»ç»“: {summary['passed_benchmarks']}/{summary['total_benchmarks']} é€šè¿‡")
    print(f"æˆåŠŸç‡: {summary['success_rate']*100:.1f}%") 